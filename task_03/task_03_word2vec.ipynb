{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from scipy import spatial\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "The goal of this project is to obtain the vector representations for words from text.\n",
    "\n",
    "The main idea is that words appearing in similar contexts have similar meanings. Because of that, word vectors of similar words should be close together. Models that use word vectors can utilize these properties, e.g., in sentiment analysis a model will learn that \"good\" and \"great\" are positive words, but will also generalize to other words that it has not seen (e.g. \"amazing\") because they should be close together in the vector space.\n",
    "\n",
    "Vectors can keep other language properties as well, like analogies. The question \"a is to b as c is to ...?\", where the answer is d, can be answered by looking into word vector space and calculating $\\mathbf{u}_b - \\mathbf{u}_a + \\mathbf{u}_c$, and finding the word vector that is the closest to the result.\n",
    "\n",
    "We are given a text that contains $N$ unique words $\\{ x_1, ..., x_N \\}$. We will focus on the Skip-Gram model in which the goal is to predict the context window $S = \\{ x_{i-l}, ..., x_{i-1}, x_{i+1}, ..., x_{i+l} \\}$ from current word $x_i$, where $l$ is the window size. \n",
    "\n",
    "We get a word embedding $\\mathbf{u}_i$ by multiplying the matrix $\\mathbf{U}$ with a one-hot representation $\\mathbf{x}_i$ of a word $x_i$. Then, to get output probabilities for context window, we multiply this embedding with another matrix $\\mathbf{V}$ and apply softmax. The objective is to minimize the loss: $-\\mathop{\\mathbb{E}}[P(S|x_i;\\mathbf{U}, \\mathbf{V})]$.\n",
    "\n",
    "You are given a dataset with positive and negative reviews. Your task is to:\n",
    "+ Construct input-output pairs corresponding to the current word and a word in the context window\n",
    "+ Implement forward and backward propagation with parameter updates for Skip-Gram model\n",
    "+ Train the model\n",
    "+ Test it on word analogies and sentiment analysis task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load data\n",
    "\n",
    "We'll be working with a subset of reviews for restaurants in Las Vegas. The reviews that we'll be working with are either 1-star or 5-star. You can download the used data set (`task03_data.npy`) from:\n",
    "\n",
    "* ([download link](https://syncandshare.lrz.de/dl/fi7cjApuE3Bd3xyfsyx3k9jr/task03_data.npy)) the preprocessed set of 1-star and 5-star reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"task03_data.npy\", allow_pickle=True)\n",
    "reviews_1star = [[x.lower() for x in s] for s in data.item()[\"reviews_1star\"]]\n",
    "reviews_5star = [[x.lower() for x in s] for s in data.item()[\"reviews_5star\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the vocabulary by taking the top 500 words by their frequency from both positive and negative sentences. We could also use the whole vocabulary, but that would be slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [x for s in reviews_1star + reviews_5star for x in s]\n",
    "vocabulary, counts = zip(*Counter(vocabulary).most_common(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = len(vocabulary)\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews: 1000\n",
      "Number of negative reviews: 2000\n",
      "Number of unique words: 500\n"
     ]
    }
   ],
   "source": [
    "print('Number of positive reviews:', len(reviews_1star))\n",
    "print('Number of negative reviews:', len(reviews_5star))\n",
    "print('Number of unique words:', VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to create two dictionaries: `word_to_ind` and `ind_to_word` so we can go from text to numerical representation and vice versa. The input into the model will be the index of the word denoting the position in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement\n",
    "---------\n",
    "word_to_ind: dict\n",
    "    The keys are words (str) and the value is the corresponding position in the vocabulary\n",
    "ind_to_word: dict\n",
    "    The keys are indices (int) and the value is the corresponding word from the vocabulary\n",
    "ind_to_freq: dict\n",
    "    The keys are indices (int) and the value is the corresponding count in the vocabulary\n",
    "\"\"\"\n",
    "### YOUR CODE HERE ###\n",
    "word_to_ind={}\n",
    "ind_to_word={}\n",
    "ind_to_freq={}\n",
    "for i in range(0,len(vocabulary)):\n",
    "    word_to_ind[str(vocabulary[i])] = i\n",
    "    ind_to_word[i] = str(vocabulary[i])\n",
    "    ind_to_freq[i] = counts[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word \"the\" is at position 0 appearing 2017 times\n"
     ]
    }
   ],
   "source": [
    "print('Word \\\"%s\\\" is at position %d appearing %d times' % \n",
    "      (ind_to_word[word_to_ind['the']], word_to_ind['the'], ind_to_freq[word_to_ind['the']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create word pairs\n",
    "\n",
    "We need all the word pairs $\\{ x_i, x_j \\}$, where $x_i$ is the current word and $x_j$ is from its context window. These will correspond to input-output pairs. We want them to be represented numericaly so you should use `word_to_ind` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window(sentence, window_size):\n",
    "    sentence = [x for x in sentence if x in vocabulary]\n",
    "    pairs = []\n",
    "\n",
    "    \"\"\"\n",
    "    Iterate over all the sentences\n",
    "    Take all the words from (i - window_size) to (i + window_size) and save them to pairs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence: list\n",
    "        A list of sentences, each sentence containing a list of words of str type\n",
    "    window_size: int\n",
    "        A positive scalar\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pairs: list\n",
    "        A list of tuple (word index, word index from its context) of int type\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    for context in range(0,len(sentence)):\n",
    "        for i in range(-window_size,window_size+1):\n",
    "            if(i!=0 and (context+i>=0) and (context+i<len(sentence))):\n",
    "                pairs.append(tuple([word_to_ind[str(sentence[context])],word_to_ind[str(sentence[context+i])]])) \n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 pairs: [[10, 6], [10, 64], [10, 320], [6, 10], [6, 64]]\n",
      "Total number of pairs: 152322\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for x in reviews_1star + reviews_5star:\n",
    "    data += get_window(x, window_size=3)\n",
    "data = np.array(data)\n",
    "\n",
    "print('First 5 pairs:', data[:5].tolist())\n",
    "print('Total number of pairs:', data.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate a weighting score to counter the imbalance between the rare and frequent words. Rare words will be sampled more frequently. See https://arxiv.org/pdf/1310.4546.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = [1 - np.sqrt(1e-3 / ind_to_freq[x]) for x in data[:,0]]\n",
    "probabilities /= np.sum(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model definition\n",
    "\n",
    "In this part you should implement forward and backward propagation together with update of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding():\n",
    "    def __init__(self, N, D, seed=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        N: int\n",
    "            Number of unique words in the vocabulary\n",
    "        D: int\n",
    "            Dimension of the word vector embedding\n",
    "        seed: int\n",
    "            Sets the random seed, if omitted weights will be random\n",
    "        \"\"\"\n",
    "\n",
    "        self.N = N\n",
    "        self.D = D\n",
    "        \n",
    "        self.init_weights(seed)\n",
    "    \n",
    "    def init_weights(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        \"\"\"\n",
    "        We initialize weight matrices U and V of dimension (D, N) and (N, D) respectively\n",
    "        \"\"\"\n",
    "        self.U = np.random.normal(0, np.sqrt(2 / self.D / self.N), (self.D, self.N))\n",
    "        self.V = np.random.normal(0, np.sqrt(2 / self.D / self.N), (self.N, self.D))\n",
    "\n",
    "    def one_hot(self, x, N):\n",
    "        \"\"\"\n",
    "        Given a vector returns a matrix with rows corresponding to one-hot encoding\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: array\n",
    "            M-dimensional vector containing integers from [0, N]\n",
    "        N: int\n",
    "            Number of posible classes\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        one_hot: array\n",
    "            (N, M) matrix where each column is N-dimensional one-hot encoding of elements from x \n",
    "        \"\"\"\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "        x = np.array([1, 0, 2])\n",
    "        one_hot = np.zeros((N,x.shape[0]))\n",
    "        one_hot[x,np.arange(x.shape[0])] = 1\n",
    "\n",
    "        assert one_hot.shape == (N, x.shape[0])\n",
    "        return one_hot\n",
    "\n",
    "    def loss(self, y, prob):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        y: array\n",
    "            (N, M) matrix of M samples where columns are one-hot vectors for true values\n",
    "        prob: array\n",
    "            (N, M) column of M samples where columns are probabily vectors after softmax\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss: int\n",
    "            Cross-entropy loss calculated as: 1 / M * sum_i(sum_j(y_ij * log(prob_ij)))\n",
    "        \"\"\"\n",
    "        #-1 / M * sum_i(sum_j(y_ij * log(prob_ij)))\n",
    "        \n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        loss = -1/prob.shape[0]*np.sum(y*np.log(prob))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def softmax(self, x, axis):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: array\n",
    "            A non-empty matrix of any dimension\n",
    "        axis: int\n",
    "            Dimension on which softmax is performed\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y: array\n",
    "            Matrix of same dimension as x with softmax applied to 'axis' dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        # subtract the max for numerical stability\n",
    "        x = x - np.expand_dims(np.max(x, axis = axis), axis)\n",
    "        x = np.exp(x)\n",
    "        y = x / np.sum(x, axis=axis)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def step(self, x, y, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        Performs forward and backward propagation and updates weights\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: array\n",
    "            M-dimensional mini-batched vector containing input word indices of int type\n",
    "        y: array\n",
    "            Output words, same dimension and type as 'x'\n",
    "        learning_rate: float\n",
    "            A positive scalar determining the update rate\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        loss: float\n",
    "            Cross-entropy loss\n",
    "        d_U: array\n",
    "            Partial derivative of loss w.r.t. U\n",
    "        d_V: array\n",
    "            Partial derivative of loss w.r.t. V\n",
    "        \"\"\"\n",
    "        \n",
    "        # Input transformation\n",
    "        \"\"\"\n",
    "        Input is represented with M-dimensional vectors\n",
    "        We convert them to (N, M) matrices such that columns are one-hot \n",
    "        representations of the input\n",
    "        \"\"\"\n",
    "        x = self.one_hot(x, self.N)\n",
    "        y = self.one_hot(y, self.N)\n",
    "\n",
    "        \n",
    "        # Forward propagation\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        embedding: array\n",
    "            (D, M) matrix where columns are word embedding from U matrix\n",
    "        logits: array\n",
    "            (N, M) matrix where columns are output logits\n",
    "        prob: array\n",
    "            (N, M) matrix where columns are output probabilities\n",
    "        \"\"\"        \n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        #  (D, M) =  (D, N)*(N, M)\n",
    "        embedding = self.U.dot(x)\n",
    "        \n",
    "        #  (N, M) =  (N, D)*(D, M)\n",
    "        logits = self.V.dot(embedding)\n",
    "        \n",
    "        #  (N, M) \n",
    "        prob = self.softmax(logits, axis=0)\n",
    "\n",
    "        \n",
    "        assert embedding.shape == (self.D, x.shape[1])\n",
    "        assert logits.shape == (self.N, x.shape[1])\n",
    "        assert prob.shape == (self.N, x.shape[1])\n",
    "    \n",
    "    \n",
    "        # Loss calculation\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        loss: int\n",
    "            Cross-entropy loss using true values and probabilities\n",
    "        \"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        loss = self.loss(y, prob)\n",
    "        \n",
    "        # Backward propagation\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        d_U: array\n",
    "            (N, D) matrix of partial derivatives of loss w.r.t. U\n",
    "        d_V: array\n",
    "            (D, N) matrix of partial derivatives of loss w.r.t. V\n",
    "        \"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "        #((N, M)) -> Gradiant of Softmax\n",
    "        d_softmax=prob - y\n",
    "        d_softmax /= prob.shape[0]\n",
    "        \n",
    "        #  (N, D) = ((N, M))*(M, D))\n",
    "        d_V = d_softmax.dot(embedding.T)\n",
    "        \n",
    "        #(D, N) =  (N, M)*((N, D) => (D,M)) * (M,N))\n",
    "        d_U = (d_softmax.T.dot(self.V)).T.dot(x.T)\n",
    "        \n",
    "        assert d_V.shape == (self.N, self.D)\n",
    "        assert d_U.shape == (self.D, self.N)\n",
    "\n",
    "        \n",
    "        # Update the parameters\n",
    "        \"\"\"\n",
    "        Updates the weights with gradient descent such that W_new = W - alpha * dL/dW, \n",
    "        where alpha is the learning rate and dL/dW is the partial derivative of loss w.r.t. \n",
    "        the weights W\n",
    "        \"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        self.U = self.U - d_U*learning_rate\n",
    "        self.V = self.V - d_V*learning_rate\n",
    "        return loss, d_U, d_V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Gradient check\n",
    "\n",
    "The following code checks whether the updates for weights are implemented correctly. It should run without an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients checked - all good!\n"
     ]
    }
   ],
   "source": [
    "def get_loss(model, old, variable, epsilon, x, y, i, j):\n",
    "    delta = np.zeros_like(old)\n",
    "    delta[i, j] = epsilon\n",
    "\n",
    "    model.init_weights(seed=132) # reset weights\n",
    "    setattr(model, variable, old + delta) # change one weight by a small amount\n",
    "    loss, _, _ = model.step(x, y) # get loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "def gradient_check_for_weight(model, variable, i, j, k, l):\n",
    "    x, y = np.array([i]), np.array([j]) # set input and output\n",
    "    \n",
    "    old = getattr(model, variable)\n",
    "    \n",
    "    model.init_weights(seed=132) # reset weights\n",
    "    _, d_U, d_V = model.step(x, y) # get gradients with backprop\n",
    "    grad = { 'U': d_U, 'V': d_V }\n",
    "    \n",
    "    eps = 1e-4\n",
    "    loss_positive = get_loss(model, old, variable, eps, x, y, k, l) # loss for positive change on one weight\n",
    "    loss_negative = get_loss(model, old, variable, -eps, x, y, k, l) # loss for negative change on one weight\n",
    "    \n",
    "    true_gradient = (loss_positive - loss_negative) / 2 / eps # calculate true derivative wrt one weight\n",
    "\n",
    "    assert abs(true_gradient - grad[variable][k, l]) < 1e-5 # require that the difference is small\n",
    "\n",
    "def gradient_check():\n",
    "    N, D = VOCABULARY_SIZE, EMBEDDING_DIM\n",
    "    model = Embedding(N, D)\n",
    "\n",
    "    # check for V\n",
    "    for _ in range(20):\n",
    "        i, j, k = [np.random.randint(0, d) for d in [N, N, D]] # get random indices for input and weights\n",
    "        gradient_check_for_weight(model, 'V', i, j, i, k)\n",
    "\n",
    "    # check for U\n",
    "    for _ in range(20):\n",
    "        i, j, k = [np.random.randint(0, d) for d in [N, N, D]]\n",
    "        gradient_check_for_weight(model, 'U', i, j, k, i)\n",
    "\n",
    "    print('Gradients checked - all good!')\n",
    "\n",
    "gradient_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our model using stochastic gradient descent. At every step we sample a mini-batch from data and update the weights.\n",
    "\n",
    "The following function samples words from data and creates mini-batches. It subsamples frequent words based on previously calculated probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, size, prob):\n",
    "    i = np.random.choice(data.shape[0], size, p=prob)\n",
    "    return data[i, 0], data[i, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model can take some time so plan accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10000 Loss: 0.037289173595415774\n",
      "Iteration: 20000 Loss: 0.03728821644809157\n",
      "Iteration: 30000 Loss: 0.03728726020394729\n",
      "Iteration: 40000 Loss: 0.037286303336207416\n",
      "Iteration: 50000 Loss: 0.03728534431709883\n",
      "Iteration: 60000 Loss: 0.03728438161541157\n",
      "Iteration: 70000 Loss: 0.0372834136940539\n",
      "Iteration: 80000 Loss: 0.03728243900759835\n",
      "Iteration: 90000 Loss: 0.0372814559998142\n",
      "Iteration: 100000 Loss: 0.037280463101182826\n",
      "Iteration: 110000 Loss: 0.037279458726391844\n",
      "Iteration: 120000 Loss: 0.03727844127180411\n",
      "Iteration: 130000 Loss: 0.037277409112897404\n",
      "Iteration: 140000 Loss: 0.03727636060167083\n",
      "Iteration: 150000 Loss: 0.037275294064013795\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "model = Embedding(N=VOCABULARY_SIZE, D=EMBEDDING_DIM)\n",
    "\n",
    "losses = []\n",
    "\n",
    "MAX_ITERATIONS = 150000\n",
    "PRINT_EVERY = 10000\n",
    "\n",
    "for i in range(MAX_ITERATIONS):\n",
    "    x, y = get_batch(data, 128, probabilities)\n",
    "    loss, _, _ = model.step(x, y, 1e-3)\n",
    "    losses.append(loss)\n",
    "\n",
    "    if (i + 1) % PRINT_EVERY == 0:\n",
    "        print('Iteration:', i + 1, 'Loss:', np.mean(losses[-PRINT_EVERY:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding matrix is given by $\\mathbf{U}^T$, where the $i$th row is the vector for $i$th word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix = model.U.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Analogies\n",
    "\n",
    "As mentioned before, vectors can keep some language properties like analogies. Given a relation a:b and a query c, we can find d such that c:d follows the same relation. We hope to find d by using vector operations. In this case, finding the real word vector $\\mathbf{u}_d$ closest to $\\mathbf{u}_b - \\mathbf{u}_a + \\mathbf{u}_c$ gives us d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go is to going as come is to [name|right|all|brought|we]\n",
      "\n",
      "look is to looking as come is to [bit|first|business|there|clean]\n",
      "\n",
      "you is to their as we is to [visit|salt|another|check|and]\n",
      "\n",
      "what is to that as when is to [chicken|days|your|here|close]\n",
      "\n",
      "go is to went as is is to [times|just|about|sat|extra]\n",
      "\n",
      "go is to went as find is to [about|just|portions|ramen|extra]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "triplets = [['go', 'going', 'come'], ['look', 'looking', 'come'], ['you', 'their', 'we'], \n",
    "            ['what', 'that', 'when'], ['go', 'went', 'is'], ['go', 'went', 'find']]\n",
    "    \n",
    "for triplet in triplets:\n",
    "    a, b, c = triplet\n",
    "    sim_w=[]\n",
    "    candidates=[]\n",
    "\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    candidates: list\n",
    "        A list of 5 closest words, measured with cosine similarity, to the vector u_b - u_a + u_c\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    \n",
    "    a_ind = word_to_ind[a]\n",
    "    b_ind = word_to_ind[b]\n",
    "    c_ind = word_to_ind[c]\n",
    "    \n",
    "    #u_b - u_a + u_c\n",
    "    vec = emb_matrix[b_ind]-emb_matrix[a_ind]+emb_matrix[c_ind]\n",
    "\n",
    "    for (i, w) in enumerate(emb_matrix):\n",
    "        if(i!=a_ind and i!=b_ind and i!=c_ind):\n",
    "            cos_sim = np.dot(vec, w)/(np.linalg.norm(vec)*np.linalg.norm(w)) #cosine similarity\n",
    "            sim_w.append(tuple((cos_sim,i)))\n",
    "    \n",
    "    sim_w.sort(key=lambda tup: tup[0], reverse=True)\n",
    "    \n",
    "    for w in sim_w[:5]:\n",
    "        candidates.append(ind_to_word[w[1]])\n",
    "    \n",
    "    print('%s is to %s as %s is to [%s]' % (a, b, c, '|'.join(candidates)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "Our end goal is to use the pretrained word vectors on some downstream task, e.g. sentiment analysis. We first generate a dataset where we just concatenate 1 and 5-star reviews into `all_sentences`. We also create a list `Y` with labels 1 for positive reviews and 0 for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 412 354  21   5 173 121 305  18 296   0 188  20 370  97 278  15  44\n",
      " 100 221  55  63 221   1   2  52 136  28  39   1  27 266   5  71   1 325\n",
      "   2  18  31   2 141  41   0 194  90 429 197   3  39   6   1  34   0  23\n",
      " 137  93   1   3 211   3   1   2   2  10  10 303  27  29   3 106   4  37\n",
      "  38 193   0   8  61  27   2 269   1   1   0   2  13 433 485  71  18 294\n",
      "  56  19  68   0 101   4   5   2 264 103 303 120   1 366  10   1   5  94\n",
      "  47  93  52 228 115   4   7 403   4   2 194 455  10 254  19   1   5  81\n",
      " 438   5]\n"
     ]
    }
   ],
   "source": [
    "all_sentences = reviews_1star + reviews_5star\n",
    "Y = np.array([0] * len(reviews_1star) + [1] * len(reviews_5star))\n",
    "\n",
    "SENTENCES_SIZE = len(all_sentences)\n",
    "MAX_SENTENCE_LENGTH = max([len(x) for x in all_sentences])\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to create an array $\\mathbf{X}$ where (i,j,k) element denotes $k$th value of an embedding for $j$th word in $i$th sentence in the dataset. In addition, we need a list that keeps track of how many words are in each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup_4/Identity:0\", shape=(64, 49, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Returns\n",
    "-------\n",
    "X: array\n",
    "    Array of dimensions (SENTENCES_SIZE, MAX_SENTENCE_LENGTH, EMBEDDING_DIM) where \n",
    "    the first dimension denotes the index of the sentence in the dataset and second is \n",
    "    the word index in the sentence. Sentences that are shorter than MAX_SENTENCE_LENGTH\n",
    "    are padded with zero vectors. Words that are not in the vocabulary are also \n",
    "    represented with zero vectors of EMBEDDING_DIM size.\n",
    "S: array\n",
    "    Array of SENTENCES_SIZE dimension containing the sentence lenghts\n",
    "\"\"\"\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "#batchSize = 64\n",
    "#numDimensions = 64\n",
    "\n",
    "#input_data = tf.placeholder(tf.int32, [batchSize, MAX_SENTENCE_LENGTH])\n",
    "\n",
    "#X = tf.Variable(tf.zeros([batchSize, MAX_SENTENCE_LENGTH, numDimensions]),dtype=tf.float32)\n",
    "#X = tf.nn.embedding_lookup(data,input_data)\n",
    "\n",
    "#print(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train on a subset of data, and test on remaining data. Your task is to split X, Y and S into training and test set (60%-40%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReturns\\n-------\\nX_train, y_train, s_train: arrays\\n    Randomly selected 60% of all data\\nX_test, y_test, s_test: arrays\\n    Rest of the data\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Returns\n",
    "-------\n",
    "X_train, y_train, s_train: arrays\n",
    "    Randomly selected 60% of all data\n",
    "X_test, y_test, s_test: arrays\n",
    "    Rest of the data\n",
    "\"\"\"\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "#X_train, X_test = data_split.train_test_split_shuffle(X, test_size=0.4)\n",
    "#Y_train, Y_test = data_split.train_test_split_shuffle(Y, test_size=0.4)\n",
    "#S_train, S_test = data_split.train_test_split_shuffle(Y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM implementation in tensorflow. Inputs are padded sequences of word vectors, sentence lengths, and true labels (0 or 1). The model takes word vectors and passes them through the LSTM. Final state is used as an input of one fully connected layer with output dimension 1. We also get probability that the class is positive and argmax label. Network uses Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, cell_dim=64):\n",
    "        \"\"\"\n",
    "        Attributes\n",
    "        ----------\n",
    "        x: float\n",
    "            Input sentence of shape (BATCH SIZE, MAX SENTENCE LENGTH, EMBEDDING DIM)\n",
    "        y: float\n",
    "            Output label of shape (BATCH SIZE)\n",
    "        s: float\n",
    "            Length of sentences of shape (BATCH SIZE)\n",
    "        last_state: float\n",
    "            The last state of sequences with shape (BATCH SIZE, CELL DIM)\n",
    "        logits: float\n",
    "            The \n",
    "        prob: float\n",
    "            Probabilities after sigmoid\n",
    "        y_hat: int\n",
    "            Predicted class value (0 or 1)\n",
    "        loss: float\n",
    "            Cross entropy loss\n",
    "        optimize:\n",
    "            Operation that updates the weights based on the loss\n",
    "        accuracy: float\n",
    "            Accuracy of prediction y_hat given y\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Define input placeholders x, y and s as class attributes\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ### \n",
    "        batchSize = 24\n",
    "        lstmUnits = 64\n",
    "        numClasses = 2\n",
    "        maxSeqLength = 500\n",
    "        dropOutRate = 0.20\n",
    "        \n",
    "        self.x = tf.placeholder(tf.int32, [batchSize, MAX_SENTENCE_LENGTH, cell_dim])\n",
    "        self.y = tf.placeholder(tf.int32, [batchSize, numClasses])\n",
    "        self.s = tf.placeholder(tf.float32, [batchSize])        \n",
    "\n",
    "        \"\"\" \n",
    "        Use dynamic_rnn to define an LSTM layer\n",
    "        Define last_state as class attribute to be the last output h of LSTM\n",
    "        (Note that we have zero padding)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ### \n",
    "        lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "        lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "        value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)       \n",
    "        \n",
    "        \"\"\"\n",
    "        Define logits, prob and y_hat as class attributes. \n",
    "        We get logits by applying a single dense layer on the last state.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ### \n",
    "        weight = tf.Variable(tf.random_normal([lstmSize, numClasses], stddev=0.1), name = \"weight\")\n",
    "        \n",
    "        bias = tf.Variable(tf.random_normal([numClasses], stddev=0.1), name = \"bias\" )\n",
    "        \n",
    "        self.logits = (tf.matmul(self.last_state, weight) + bias)\n",
    "        \n",
    "        self.prob = \n",
    "        \n",
    "        self.y_hat = \n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.y, logits=self.logits))\n",
    "        self.optimize = tf.train.AdamOptimizer().minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we finally train our RNN model and evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 100 Train loss: 0.55767053 Train accuracy: 0.71875\n",
      "Iter: 200 Train loss: 0.5156001 Train accuracy: 0.6875\n",
      "Iter: 300 Train loss: 0.46760798 Train accuracy: 0.796875\n",
      "Test loss: 0.5906389 Test accuracy: 0.7316666666666667\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "model = LSTM()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for iter in range(300):\n",
    "        i = np.random.randint(0, X_train.shape[0], 64)\n",
    "        feed = { model.x: X_train[i], model.y: y_train[i], model.s: s_train[i] }\n",
    "        _ = sess.run(model.optimize, feed)\n",
    "        \n",
    "        if (iter + 1) % 100 == 0:\n",
    "            train_loss, train_accuracy = sess.run([model.loss, model.accuracy], feed)\n",
    "            print('Iter:', iter + 1, 'Train loss:', train_loss, 'Train accuracy:', train_accuracy)\n",
    "\n",
    "    test_loss, test_pred = sess.run([model.loss, model.y_hat], { model.x: X_test, model.y: y_test, model.s: s_test })\n",
    "    print('Test loss:', test_loss, 'Test accuracy:', np.mean(test_pred == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
